---
title: "Marketing MultiMix Model For VMWare"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 1}

#-----------------------------------------------------------------------------------------------------------
training_path="C:/Users/karan/iCloudDrive/i UIC MSBA/Actual Academics/Semester 1/IDS 572 Data Mining for Business/Assignments/HW3/Data/Training.csv"
validation_path="C:/Users/karan/iCloudDrive/i UIC MSBA/Actual Academics/Semester 1/IDS 572 Data Mining for Business/Assignments/HW3/Data/Validation.csv"
Training <- read.csv(training_path)
Validation<-read.csv(validation_path)
library(dplyr)
library(plyr)
library(purrr)
glimpse(Training)

colname<-colnames(Training)

###########################################################################################################
##################----------------------Data cleaning-----------------------------------###################
###########################################################################################################

#--------------------Percentage of Missing Values - NA values-----------------------------------------------
miss_pct<-function(x){
  return((sum(complete.cases(x)) / nrow(Training))*100)
}

complete_data<-sapply(Training, miss_pct)
miss<-data.frame(colname,complete_data)
View(miss)

col_rem<-which(miss[,2]<30) #--- Threshold -> columns with more than 70% NA values dropped
length(col_rem)

Training<-Training[,-col_rem]
dim(Training)
#---------------Percentage of 9999 values-------------------------------------------------------------------

pct_9999<-function(x){
  ab<-(which(x==9999))
  return(length(ab)/nrow(Training) * 100)
}
train_9999_rm<-sapply(Training, pct_9999)
View(train_9999_rm)

outlr_9999<-data.frame(colnames(Training),train_9999_rm)
View(outlr_9999)

col_drop_9999<-(which(outlr_9999[,2]>70)) # Threshold -> columns with more than 75% 9999 values dropped
train_f1<-Training[,-col_drop_9999]
train_f1<-data.frame(train_f1)

dim(train_f1)
grep("target",colnames(train_f1))
train_f1$target=as.factor(train_f1$target)

#-------------------Unknown Values-------------------------------------------------------------------------

pct_unknown<-function(x){
  ab<-(which(x=="Unknown"))
  return(length(ab)/nrow(train_f1) * 100)
}
train_unk_rm<-sapply(train_f1, pct_unknown)
train_unk_rm<-data.frame(train_unk_rm)
View(train_unk_rm)

unkn_thresh<-(which(train_unk_rm[,1]>50)) # Threshold -> columns with more than 70% Unknown values dropped
unkn_thresh

train_f2<-data.frame(train_f1[,-unkn_thresh])
dim(train_f2)
#-------------------------------NO Variance or SD=0---------------------------------------------------------------
t<-c()

for(i in 1: ncol(train_f2)){
  
  if(is.factor(train_f2[,i])==TRUE)
  {
    t<-append(t,i)
  }
}
train_f2_wo<-data.frame(train_f2[,-t])
train_f2_w<-train_f2[,t]
train_f2_w<-train_f2_w[,-8]

sd_check<-function(x){
  
  return(sd(x,na.rm=T))
}

sd_rm<-sapply(train_f2_wo, sd_check)
sd_rm<-data.frame(sd_rm)

l1<-which(sd_rm[,1]==0)
length(l1)

train_f3<-train_f2_wo[,-l1]
train_f3<-data.frame(train_f3)

dim(train_f3)
target<-train_f2$target
train_f3<-data.frame(train_f3,target)

grep("target",colnames(train_f3))

train_f3$target<-as.factor(train_f3$target)
summary(train_f3$target)

glimpse(train_f3)

#---------------------------------------------------------------------------------------------------------------
#--------------Imputing -----------------------------------------------------------------------------------#####
#----------Imputing Median to replace NA values---------------#

imput<-function(x){
  x1<-c()
  if(is.factor(x)==F)
  {
    x1<-(ifelse(is.na(x),median(x,na.rm = T),x))
  }
  else
  {
    x1=x
  }
  return(x1)
}

temp<-c()
for(i in 1: ncol(train_f3)){
  
  if(is.factor(train_f3[,i])==TRUE)
  {
    temp<-append(temp,i)
  }
}
train_f4<-sapply(train_f3, imput)
train_f4<-data.frame(train_f4,train_f2_w)
grep("target",colnames(train_f4))
train_f4$target<-as.factor(train_f4$target)
dim(train_f4)




# ################################################################################################################

# #####################-------------------Subset Selection using Burato-------------------------##################
# ################################################################################################################
library(MASS)
# 
# library(Boruta)
# 
tem1<-c()
for(i in 1: (ncol(train_f4))){
  
  if(is.factor(train_f4[,i])==TRUE)
  {
    tem1<-append(tem1,i)
  }
}
# 
# 
train_num<-data.frame(train_f4[,-tem1])
train_num<-data.frame(train_num[,-1])
train_categ<-data.frame(train_f4[,tem1])
# 
# ##----------------Removing highly correlated columns with target---------------------------------------------#

train_num$target.1<-as.numeric(train_f4$target)
grep("target.1", colnames(train_num))
# 
cor_matrx <- data.frame(colnames(train_num[,-437]),cor(train_num[,-437], train_num$target.1))
remove_cor <- which(cor_matrx[,2] > 0.60 & cor_matrx[,2] != 1)
length(remove_cor)
#length(cor_op)
# 
train_num <- train_num[,-remove_cor]
# glimpse(train_num)
train_num$target.1<-as.factor(train_num$target.1)
# 
# #-----------------------Boruta -----------------------------------------------------------------------------#
# set.seed(555)
# boruta_op<-Boruta(target~.,data =train_num, doTrace=2, maxRuns=300)
# plot(boruta_op,las=2)
# 
# plot(boruta_op)
# 
# boruta_op_matr<-data.frame(colnames(train_num[,-437]),boruta_op$finalDecision)
# dim(boruta_op_matr)
# 
# burato_op_features<-boruta_op_matr[which(boruta_op_matr[,2]!="Rejected"),]
# colnames(burato_op_features)<-c("column","status")
# 
c11<-c("tot_page_views",	"tot_page_views_l30d",	"tot_page_views_l90d",	"tot_visits",	"tot_visits_l30d",	"tot_visits_l90d",	"log_in_events",	"product_view_events",	"checkout_s1_events",	"purchase_events",	"natural_search_events",	"file_download_events",	"tot_google_browser_page_views",	"tot_internal_ref_page_views",	"tot_search_engine_ref_page_views",	"tot_other_ref_page_views",	"tot_windows_page_views",	"tot_google_se_page_views",	"tot_prod10_blog_page_views",	"tot_first_touch_direct_views",	"tot_last_touch_natural_search_views",	"tot_last_touch_direct_views",	"tot_last_touch_internal_views",	"tot_last_touch_referring_domain_views",	"pdf_downloads",	"tot_prod10_downloads",	"tgt_hol",	"tgt_webinar",	"tgt_whitepaper",	"tgt_download",	"masked_email",	"ftr_dummy_db_industryUnknown",	"tgt_first_date_hol_page_view",	"tgt_first_date_eval_page_view",	"tgt_first_date_webinar_page_view",	"tgt_first_date_whitepaper_download",	"tgt_first_date_any_download",	"tgt_more_than1")
c11<-data.frame(c11)

ind2<-c()
get_col_index<-function(x){
  
  ind2<-which(colnames(train_num)==x)
}
burato_ncols<-c11$c11
burato_op_index<-sapply(burato_ncols,get_col_index)
burato_op_num<-train_num[,unlist(burato_op_index)]

View(colnames(burato_op_num))
```

### Burato Outpur Showing Feature Importance:
```{r c, out.width="32.8%", fig.show="hold"}
knitr::include_graphics("C:/Users/karan/iCloudDrive/i UIC MSBA/Actual Academics/Semester 1/IDS 572 Data Mining for Business/Assignments/HW3/Data/plot_zoom_png.png")
```

```{r d}
################################################################################################################
#####################------------------Subset selection using Ranger------------------------####################
################################################################################################################
#install.packages("ranger")
library(ranger)
set.seed(555)
ranger_op<-ranger(formula = target.1~.,data = train_num,num.trees = 500,verbose = T,importance = "impurity")
ranger_op_df<-data.frame(colnames(train_num[,-435]),ranger_op$variable.importance)
colnames(ranger_op_df)<-c("column","importance")
top_100_ranger<-data.frame(top_n(ranger_op_df[with(ranger_op_df,order(-ranger_op_df$importance)),],100))

ranger_top_100_cols<-top_100_ranger[,1]

#importance_pvalues(ranger_op, method = "altmann", formula = target ~ ., data =train_num )

ranger_op_index<-sapply(ranger_top_100_cols,get_col_index)
ranger_op_num<-train_num[,ranger_op_index]
dim(ranger_op_num)

################################################################################################################
#--------------Complete Burato and Ranger Dataset-------------------------------------------------------------#
###############################################################################################################
grep("target",colnames(train_f4))
target<-train_f4$target

View(train_categ)
#----------Replacing NA to Miss-------------------------------------------------------------------------------##
replace1<-function(x){
  
  `levels<-`(addNA(x), c(levels(x), 'Missing'))
  #levels(x)<-c(levels(x),"Missing")
  #ifelse(is.na(x),"Missing",x)
}

train_categ<-data.frame(train_categ)

train_categ<-data.frame((sapply(train_categ, replace1)))
Burato_Train<-data.frame(burato_op_num,train_categ,target)
Ranger_Train<-data.frame(ranger_op_num,train_categ,target)

grep("target",colnames(Ranger_Train))
Burato_Train$target<-as.factor(Burato_Train$target)
Ranger_Train$target<-as.factor(Ranger_Train$target)

View(Ranger_Train)

################################################################################################################
#####################-------------------Class Balancing--------------------------------------###################
################################################################################################################

prop.table(table(Burato_Train$target))

dim(Burato_Train)
#----------------------------------------UPSCALE------------------------------------------------------------#
library(caret)
Ranger_up_train<-upSample(x=Ranger_Train[,-grep("target",colnames(Ranger_Train))],y=Ranger_Train$target)
Burato_up_train<-upSample(x=Burato_Train[,-grep("target",colnames(Burato_Train))],y=Burato_Train$target)
table(Ranger_up_train$Class)
table(Burato_up_train$Class)
dim(Ranger_up_train)
dim(Burato_up_train)

#----------------------------------------SMOTE------------------------------------------------------------#
library(UBL)

smote_ranger_up_train<-SmoteClassif(target~.,Ranger_Train, "balance",k=5,dist = "HEOM")
smote_burato_up_train<-SmoteClassif(target~.,Burato_Train, "balance",k=5,dist = "HEOM")
dim(smote_ranger_up_train)
dim(smote_burato_up_train)


View(Burato_Train)
##############################################################################################################
#######---------------------Normalizing Numerical Values--------------------------------------------##########
##############################################################################################################

#-----------------UPSAMPLE------------------------------------------------------------------------------------#
Burato_up_norm<-scale(Burato_up_train[,c(1:(ncol(Burato_up_train) - ncol(train_categ)-1))],center = T,scale = T)
Ranger_up_norm<-scale(Ranger_up_train[,c(1:(ncol(Ranger_up_train) - ncol(train_categ)-1))],center = T,scale = T)

#---------------SMOTE-----------------------------------------------------------------------------------------#

Burato_up_norm_smote<-data.frame(scale(smote_burato_up_train[,c(1:(ncol(smote_burato_up_train) - ncol(train_categ)-1))],center = T,scale = T))
Ranger_up_norm_smote<-scale(smote_ranger_up_train[,c(1:(ncol(smote_ranger_up_train) - ncol(train_categ)-1))],center = T,scale = T)

##############################################################################################################
#######---------------------Complete Normalized Dataset---------------------------------------------##########
##############################################################################################################

Burato_up_comp<-data.frame(Burato_up_norm,Burato_up_train[,(ncol(Burato_up_norm)+1):(ncol(Burato_up_train))])
Ranger_up_comp<-data.frame(Ranger_up_norm,Ranger_up_train[,(ncol(Ranger_up_norm)+1):(ncol(Ranger_up_train))])

Burato_smote_comp<-data.frame(Burato_up_norm_smote,smote_burato_up_train[,(ncol(Burato_up_norm_smote)+1):(ncol(smote_burato_up_train))])
Ranger_smote_comp<-data.frame(Ranger_up_norm_smote,smote_ranger_up_train[,(ncol(Ranger_up_norm_smote)+1):(ncol(smote_ranger_up_train))])


summary(Ranger_smote_comp$target)
summary(Burato_smote_comp$target)
summary(Burato_up_comp$Class)
summary(Ranger_up_comp$Class)

##############################################################################################################
##############################################################################################################

##############################################################################################################
##############################################################################################################

#---------------------------------------------------------------------------------------------------------------
#-------Removing Factor variable with more than 40 classes---------------------

level_l <- data.frame(sapply(smote_burato_up_train, function(x) {
  if(!is.numeric(x)) {
    length(levels(x)) > 40  
  }
  else {
    FALSE
  }
}))
level_F <- which(level_l[,1] == TRUE)

smote_burato_up_train<-data.frame(smote_burato_up_train[,-level_F])
Validation_data<-data.frame(Validation[,-level_F])
smote_burato_up_train<-smote_burato_up_train[,-39]

Validation_data<-Validation_data[-39]
View(Validation_data)


level_l <- data.frame(sapply(Ranger_smote_comp, function(x) {
  if(!is.numeric(x)) {
    length(levels(x)) > 40  
  }
  else {
    FALSE
  }
}))
level_F <- which(level_l[,1] == TRUE)

Ranger_smote_comp<-data.frame(Ranger_smote_comp[,-level_F])
Validation_data<-data.frame(Validation[,-level_F])
Ranger_smote_comp<-Ranger_smote_comp[,-102]

#-----------------------------One hot encoding - convert categorical to numerical-----------------------------

glimpse(Ranger_smote_comp)

dummy <- dummyVars(~ ., data = Ranger_smote_comp[,-c(101,102)])
mt = predict(dummy, newdata = Ranger_smote_comp[,-102])
target<-Ranger_smote_comp[,102]
ranger_data <- data.frame(mt, target)
glimpse(ranger_data)
#----------------Test data preparation------------------------------------------------------------------------
ind3<-c()
get_col_index1<-function(x){
  
  ind3<-which(colnames(Validation)==x)
}
colname_train<-colnames(Burato_smote_comp)
Validation_index<-(sapply(colname_train, get_col_index1))
Validation_index<-unlist(Validation_index)


Validation_data<-data.frame(Validation[,Validation_index])
View(Validation_data)

colname_train1<-colnames(ranger_data)
Validation_index<-(sapply(colname_train1, get_col_index1))
Validation_index<-unlist(Validation_index)


Validation_data_rang<-data.frame(Validation[,Validation_index])

#---------------------Logistic Using Ranger----------------------------------------------------------------------------####

Xtrain<-ranger_data[,-101]
s<-scale(Xtrain,center = T,scale = T)
yTrain<-ranger_data[,101]

xtest<-Validation_data_rang[,-101]
ytest<-Validation_data_rang[,101]

#install.packages("LiblineaR")
library(LiblineaR)
Logistic_ranger<-LiblineaR(data=s,target=yTrain,type=0,cost=0.8,cross=5)

# Find the best model with the best cost parameter via 10-fold cross-validations

tryTypes <- c(0:7)
tryCosts <- c(1000,1,0.001)
bestCost <- NA
bestAcc <- 0
bestType <- NA

for(ty in tryTypes){
  for(co in tryCosts){
    acc <- LiblineaR(data=s, target=yTrain, type=ty, cost=co, cross=5, verbose=FALSE)
    cat("Results for C=",co," : ",acc," accuracy.\n",sep="")
    if(acc>bestAcc){
      bestCost <- co
      bestAcc <- acc
      bestType <- ty
    }
  }
}
bestCost
cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")

m <- LiblineaR(data=s,target=yTrain,type=bestType,cost=bestCost)
s2 <- scale(xtest,attr(s,"scaled:center"),attr(s,"scaled:scale"))
p <- predict(m,s2)

res <- table(p$predictions,ytest)
print(res)

acc<-sum(diag(res))/sum(res)
acc

#--------------L1 Norm---------------------------------------------------------------------------------------------------

set.seed(555)
m1<-LiblineaR(data=s,target=yTrain,type=6,cost=bestCost)
p1<-predict(m1,s2)
res1 <- table(p1$predictions,ytest)
acc1<-sum(diag(res1))/sum(res1)
acc1
#-------------L2 Norm-----------------------------------------------------------------------------------------------------#

set.seed(555)
m2<-LiblineaR(data=s,target=yTrain,type=0,cost=bestCost)
p2<-predict(m2,s2)
res2 <- table(p2$predictions,ytest)
acc2<-sum(diag(res2))/sum(res2)
acc2


#######################################################################################################################
#######################################################################################################################

#----------------------------------Random Forest----------------------------------------------------------------#

library(randomForest)


ranger_data$target11<-as.factor(ranger_data$target)

rf<-randomForest(target11~.,data=ranger_data,importance=T,do.trace=T,mtry=sqrt(ncol(ranger_data)-1),ntree=150)
plot(rf)

#---------Parameter Tuning-------------------------------------------------------------------#

tuned_val<-tuneRF(ranger_data[,-107],ranger_data$target11,ntreeTry = 40,improve = 0.5)

#taking mtry=20
tuned_rf_model<-randomForest(target11~.,data=ranger_data,importance=T,do.trace=T,mtry=20,ntree=40)

pred_rf<-predict(tuned_rf_model,newdata=validation_ranger_data)
cnf_mtr<-table(pred_rf,validation_ranger_data$target11)
acc<-sum(diag(cnf_mtr))/sum(cnf_mtr)

acc

#######################################################################################################################
#######################################################################################################################

#---------------------------XG-Boosting-------------------------------------------------------------------------------#
library(xgboost)
library(caret)
target <- ranger_data$target

xg_data <- ranger_data
Validation_data_rang$target<-as.factor(Validation_data_rang$target)
xg_validate<-Validation_data_rang
dim(Validation_data_rang)

xg_validate_target<-Validation_data_rang$target
xg_validate<-xg_validate[,-101]
dim(xg_validate)

target_classes <- xg_data$target
target_validate_class <- xg_validate$target
label <- as.integer(xg_data$target)-1

label_1<-as.integer(Validation_data_rang$target)-1
xg_data$target = NULL

summary(label_1)

n = nrow(xg_data)
train.index = sample(n,floor(0.75*n))
train.data = as.matrix(xg_data[train.index,])
train.label = label[train.index]

test.data = as.matrix(xg_data[-train.index,])
test.label = label[-train.index]
View(test.label)

View(colnames(xg_validate))


xg_validate <- as.matrix(xg_validate)
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)



dim(test.data)
length(test.label)
num_class1 = length(levels(target))

#----------------eta=0.001 , max_depth=5----------------------------------------------------

params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class1
)

# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=250,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=1
)

xgb.pred <- predict(xgb.fit,test.data,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(target)
xgb.pred

xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(target)[test.label+1]
xgb.pred

data.frame(xgb.pred$prediction , xgb.pred$label)
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))

View(colnames(xg_validate))

xgb.pred1<-predict(xgb.fit,xg_validate,reshape = T)
xgb.pred1<-data.frame(xgb.pred1)
colnames(xgb.pred1)<-levels(target)
xgb.pred1

xgb.pred1$prediction=apply(xgb.pred1,1,function(x) colnames(xgb.pred1)[which.max(x)])
xgb.pred1$label = levels(target)[label_1+1]
xgb.pred1

data.frame(xgb.pred1$prediction , xgb.pred1$label)
result1 = sum(xgb.pred1$prediction==xgb.pred1$label)/nrow(xgb.pred1)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result1)))

#---------------------------------Different Parameter Values for XG-Boosting-------------------------------------#

#----------------eta=0.1 , max_depth=2 Gamma =6----------------------------------------------------

params1 = list(
  booster="gbtree",
  eta=0.1,
  max_depth=2,
  gamma=6,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class1
)

# Train the XGBoost classifer
xgb.fit1=xgb.train(
  params=params1,
  data=xgb.train,
  nrounds=200,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=1
)

xgb.pred1 <- predict(xgb.fit1,test.data,reshape=T)
xgb.pred1 = as.data.frame(xgb.pred1)
colnames(xgb.pred1) = levels(target)
xgb.pred1

xgb.pred1$prediction = apply(xgb.pred1,1,function(x) colnames(xgb.pred1)[which.max(x)])
xgb.pred1$label = levels(target)[test.label+1]
xgb.pred1

data.frame(xgb.pred1$prediction , xgb.pred1$label)
result1 = sum(xgb.pred1$prediction==xgb.pred1$label)/nrow(xgb.pred1)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result1)))

View(colnames(xg_validate))

xgb.pred11<-predict(xgb.fit1,xg_validate,reshape = T)
xgb.pred11<-data.frame(xgb.pred11)
colnames(xgb.pred11)<-levels(target)
xgb.pred11

xgb.pred11$prediction=apply(xgb.pred11,1,function(x) colnames(xgb.pred11)[which.max(x)])
xgb.pred11$label = levels(target)[label_1+1]
xgb.pred11

data.frame(xgb.pred11$prediction , xgb.pred11$label)
result11 = sum(xgb.pred11$prediction==xgb.pred11$label)/nrow(xgb.pred11)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result11)))

############################################################################################################################
############################################################################################################################
############################################################################################################################
```
